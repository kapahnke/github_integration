{
    "metadata": {
        "kernelspec": {
            "name": "scala", 
            "display_name": "Scala 2.10 with Spark 1.6", 
            "language": "scala"
        }, 
        "language_info": {
            "name": "scala"
        }
    }, 
    "cells": [
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "# Model bike sharing data with SPSS\nThis Scala 2.10 notebook shows you how to create a predictive model of bike sharing trends by using IBM SPSS Algorithms on Apache Spark. You'll learn how to create a generalized linear model with the SPSS ML API, and how to view the model with the SPSS Model Viewer.\n\nThe generalized linear model (GLM) is an analytical algorithm for different types of data. It includes statistical models such as linear regression for normally distributed targets, logistic models for binary or multinomial targets, and log linear models for count data. In addition to building a model, the GLM provides features such as variable selection, automatic selection of the distribution and link function, and model evaluation statistics. The GLM has options for regularization, such as LASSO, ridge regression, and elastic net, and can handle a wide variety of data.\n\nThe bike sharing model will:\n - Identify what affects the amount of bike rentals.\n - Predict future daily bike rental amounts based on date, weather, and season. \n\nThis notebooks runs on Scala 2.10 with Spark 1.6. Some familiarity with Scala is recommended.\n\n## Table of contents \nThis notebook contains these main sections:\n\n1. [Overview of the bike sharing data](#overview)\n1. [Prepare the data](#prepare)\n1. [Configure the generalized linear model](#configure) \n1. [View the model](#view)\n1. [Summary and next steps](next)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"overview\"></a>\n## 1. Overview of the bike sharing data\n\nYou'll be looking at a the daily count of bike rentals between the years 2011 and 2012 in the Capital Bikeshare system, with corresponding weather and seasonal information. The [Capital Bikeshare](https://www.capitalbikeshare.com/home) system provides bicycles at over 400 stations in Washington, D.C. and neighboring cities in Virginia and Maryland. \n\nThe data set that you'll use has the following fields:\n\n- instant: the record ID\n- dteday: the date\n- season: the season (1 = spring, 2 = summer, 3 = fall, 4 = winter)\n- yr: the year (0 = 2011, 1 = 2012)\n- mnth: the month ( 1 - 12)\n- hr: the hour (0 - 23)\n- holiday: 0 = not a holiday, 1 = a holiday \n- weekday: the day of the week (Sunday = 0 - Friday = 6)\n- workingday: 0 = a weekend or holiday, 1 = a work day\n- weathersit: the weather conditions \n   - 1 = Clear or partly cloudy\n   - 2 = Mist or clouds\n   - 3 = Light precipitation\n   - 4 = Heavy precipitation\n- temp: the normalized temperature for the day in degrees Celsius (minimum = -8, maximum = +39) \n- atemp: the normalized feels-like temperature in degrees Celsius (minium = -16, maximum = +50) \n- hum: the normalized humidity (maximum = 100%)\n- windspeed: the normalized wind speed in knots (maximum = 67)\n- casual: the count of bikes rented to casual users\n- registered: the count of bikes rented to registered users\n- cnt: the total count of rented bikes (casual + registered)\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"prepare\"></a>\n# 2. Prepare the data\nTo prepare the bike sharing data:  \n\n1. [Get the data into your notebook](#load)\n1. [Create a Spark DataFrame](#df)\n1. [Enrich the DataFrame](#enrich)\n\n<a id=\"load\"></a>\n## 2.1. Get the data into your notebook\nTo get the data and load it into your notebook:\n\n1. Download the `Bike-Sharing-Dataset.zip` file from this website: [https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset](https://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).\n1. Extract the file.\n1. Load the `day.csv` file into the notebook by clicking the __Add and Find Data__ icon on the notebook action bar. Drop the file into the box or browse to select the file.\n\nThe file is loaded to your object storage. The data set appears in the __Files__ list in the notebook and also in the __Data Assets__ section of the project."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"df\"></a>\n## 2.2. Create a Spark DataFrame\nAfter you create an SQLContext and insert your credentials, you can create a Spark DataFrame."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Run this cell to create an SQLContext for your DataFrame:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import org.apache.spark.sql.SQLContext\nval sqlContext = new SQLContext(sc)\n\ndef setHadoopConfig(credentials: collection.mutable.Map[String, String]) = {\n    val prefix = \"fs.swift.service.\" + credentials(\"name\") \n    val hconf = sc.hadoopConfiguration\n    hconf.set(prefix + \".auth.url\", credentials(\"auth_url\") + \"/v3/auth/tokens\")\n    hconf.set(prefix + \".auth.endpoint.prefix\", \"endpoints\")\n    hconf.set(prefix + \".tenant\", credentials(\"project_id\"))\n    hconf.set(prefix + \".username\", credentials(\"user_id\"))\n    hconf.set(prefix + \".password\", credentials(\"password\"))\n    hconf.setInt(prefix + \".http.port\", 8080)\n    hconf.set(prefix + \".region\", credentials(\"region\"))\n    hconf.setBoolean(prefix + \".public\", true)\n}"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Insert your object storage credentials for the data set. Put your cursor in the following cell and click __Insert to code__, which appears after the `days.csv` file. If the number after `credentials` in the first line of code is not 1, edit the code to say `credentials_1`."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": ""
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown", 
            "source": "Run this cell to set your credentials with Spark:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "credentials_1(\"name\") = \"gle\"\nsetHadoopConfig(credentials_1)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Create the Spark DataFrame:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "val filePath = \"swift://\" + credentials_1(\"container\") + \".\" + credentials_1(\"name\") + \"/\"\nval fileName = credentials_1(\"filename\")\nval df = sqlContext.read.format(\"com.databricks.spark.csv\").\n    option(\"header\", \"true\").option(\"inferschema\", \"true\").load(filePath + fileName)\n"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"enrich\"></a>\n## 2.3. Enrich the DataFrame\n\nThe generalized linear model algorithm requires generated properties for the fields in the DataFrame so that they have proper data types, measurable levels, and roles.   \n\nRun the SPSS DataFrame assistant `enrich` function to generate those properties:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import com.ibm.spss.ml.utils.DataFrameImplicits._\nval df2 = df.enrich"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Show the first three rows of the DataFrame:"
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "df2.show(3)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"configure\"></a>\n# 3. Configure the generalized linear model \n\nConfigure the generalized linear model with the `GeneralizedLinear()` method to analyze what conditions affect the number of rented bikes. \n\nFirst, import the SPSS generalized linear model algorithm package:"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import com.ibm.spss.ml.classificationandregression.GeneralizedLinear\nimport com.ibm.spss.ml.classificationandregression.params._"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Now, run the `GeneralizedLinear()` method. You set the `TargetField` parameter to `cnt` and the `Effects` list to the fields that describe the type of day, the season, and the weather conditions. By specifying `UNKNOWN` for the distribution and link function, the model automatically chooses the most appropriate settings for the data."
        }, 
        {
            "metadata": {
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "val gle = GeneralizedLinear().\n  setTargetField(\"cnt\").\n  setInputFieldList(Array(\"season\",\"yr\",\"mnth\",\"holiday\",\"weekday\",\"workingday\",\"weathersit\",\"temp\",\"atemp\",\"hum\",\"windspeed\",\"casual\",\"registered\")).\n  setEffects(List(\n    Effect(List(\"season\"), List(0)), \n    Effect(List(\"mnth\"), List(0)),\n    Effect(List(\"holiday\"), List(0)),\n    Effect(List(\"weekday\"), List(0)),\n    Effect(List(\"workingday\"), List(0)),\n    Effect(List(\"weathersit\"), List(0)),\n    Effect(List(\"temp\"), List(0)),\n    Effect(List(\"atemp\"), List(0)),\n    Effect(List(\"hum\"), List(0)),\n    Effect(List(\"windspeed\"), List(0)))).\n  setDistribution(\"UNKNOWN\").\n  setLinkFunction(\"UNKNOWN\").      \n  setUseVariableSelection(true).\n  setVariableSelectionMethod(\"FORWARD_STEPWISE\").\n  setDetectTwoWayInteraction(true).\n  setTargetSortOrder(\"DESCENDING\")\n\nval gle_model = gle.fit(df2)"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"view\"></a>\n# 4. View the model \n\nView the model with the SPSS Model Viewer. The visualization for the generalized linear model includes tests of model effects, statistics for each parameter, and a table and chart of standardized deviation residuals.\n\n## 4.1 Generate a project token\n\nBefore you can run the model viewer, you need to generate a project token\n\n1. In the **My Projects** banner, click the **More** icon and then click **Insert project token**. The project token is inserted into the first cell of the notebook, before the title.\n2. Copy the text, which appears at the beginning of the notebook, into the following cell and run it."
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": ""
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## 4.2 Start the model viewer\n\nRun the code in the following cell to start SPSS Model Viewer, where you can see a visualization and see model statistics and other characteristics."
        }, 
        {
            "metadata": {
                "scrolled": false, 
                "collapsed": false
            }, 
            "cell_type": "code", 
            "outputs": [], 
            "execution_count": null, 
            "source": "import com.ibm.spss.scala.ModelViewer\nkernel.magics.html(ModelViewer.toHTML(pc, gle_model))"
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "<a id=\"next\"></a>\n# Summary and next steps\nYou have created a generalized linear model of the bike sharing data. Now you can:\n - Create a different model to compare model evaluations, like the test of model effects, residuals, and so on. See [SPSS documentation](https://apsportal.ibm.com/docs/content/kc_gen/integrations-gen2.html).\n - Predict further bike rental amounts for incoming data."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "## Authors\n\nKang Jiangbo and Yu Wenpei are SPSS Algorithm Engineers at IBM.\n\n### Data citations\nLichman, M. (2013). UCI Machine Learning Repository [https://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science. \n\nFanaee-T, Hadi, and Gama, Joao, 'Event labeling combining ensemble detectors and background knowledge', Progress in Artificial Intelligence (2013): pp. 1-15, Springer Berlin Heidelberg [Web link](doi:10.1007/s13748-013-0040-3)."
        }, 
        {
            "metadata": {}, 
            "cell_type": "markdown", 
            "source": "Copyright \u00a9 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
        }
    ], 
    "nbformat_minor": 0, 
    "nbformat": 4
}